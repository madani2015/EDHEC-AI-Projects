{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   numerical_var_1  9961 non-null   float64\n",
      " 1   numerical_var_2  9960 non-null   float64\n",
      " 2   numerical_var_3  9960 non-null   float64\n",
      " 3   numerical_var_4  9960 non-null   float64\n",
      " 4   categorical_var  9960 non-null   object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 390.8+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numerical_var_1</th>\n",
       "      <th>numerical_var_2</th>\n",
       "      <th>numerical_var_3</th>\n",
       "      <th>numerical_var_4</th>\n",
       "      <th>categorical_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.000000e+09</td>\n",
       "      <td>-6.784947e-01</td>\n",
       "      <td>0.348286</td>\n",
       "      <td>-1.980572</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.382643e-01</td>\n",
       "      <td>-1.000000e+09</td>\n",
       "      <td>0.283324</td>\n",
       "      <td>-1.054986</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.476885e-01</td>\n",
       "      <td>-5.973811e-01</td>\n",
       "      <td>-0.936520</td>\n",
       "      <td>-0.587028</td>\n",
       "      <td>-999999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.523030e+00</td>\n",
       "      <td>1.104180e-01</td>\n",
       "      <td>0.579584</td>\n",
       "      <td>0.149669</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.341534e-01</td>\n",
       "      <td>1.197179e+00</td>\n",
       "      <td>-1.490083</td>\n",
       "      <td>1.024162</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   numerical_var_1  numerical_var_2  numerical_var_3  numerical_var_4  \\\n",
       "0    -1.000000e+09    -6.784947e-01         0.348286        -1.980572   \n",
       "1    -1.382643e-01    -1.000000e+09         0.283324        -1.054986   \n",
       "2     6.476885e-01    -5.973811e-01        -0.936520        -0.587028   \n",
       "3     1.523030e+00     1.104180e-01         0.579584         0.149669   \n",
       "4    -2.341534e-01     1.197179e+00        -1.490083         1.024162   \n",
       "\n",
       "  categorical_var  \n",
       "0            High  \n",
       "1            High  \n",
       "2      -999999999  \n",
       "3            High  \n",
       "4            High  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "#Ensure your dataset data lesson 01.csv is in the same folder as your notebook\n",
    "file_path = \"data_lesson_01.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(data.info())\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values in each column:\n",
      "numerical_var_1    0.39\n",
      "numerical_var_2    0.40\n",
      "numerical_var_3    0.40\n",
      "numerical_var_4    0.40\n",
      "categorical_var    0.40\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#What percentage of the data is missing?\n",
    "# Calculate missing data percentage\n",
    "#.isnull() function returns a DataFrame True or False to identify missing (or null) values in a dataset.\n",
    "#True indicates a missing value (np.nan), and False indicates a non-missing value\n",
    "#.sum(): Counts the number of True values in each column (i.e., the total number of missing values in that column).\n",
    "#len(data): Returns the total number of rows in the dataset\n",
    "\n",
    "missing_percentage = (data.isnull().sum() / len(data)) * 100\n",
    "\n",
    "# Display the percentage of missing values for each column\n",
    "print(\"Percentage of missing values in each column:\")\n",
    "print(missing_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows to remove: 0\n",
      "Columns to remove: []\n"
     ]
    }
   ],
   "source": [
    "# Are there any data points that need to be removed?\n",
    "# Define a threshold for removal (e.g., rows with >30% missing values)\n",
    "threshold = 30  # Change this based on analysis needs\n",
    "\n",
    "#identifies rows where the number of missing values exceeds the threshold\n",
    "#Check rows and columns with excessive missing values\n",
    "#data.isnull().sum(axis=1): Counts the number of missing values for each row\n",
    "#(len(data.columns) * (threshold / 100)) is the allowed number of missing values based on the threshold.\n",
    "rows_to_remove = data[data.isnull().sum(axis=1) > (len(data.columns) * (threshold / 100))]\n",
    "columns_to_remove = missing_percentage[missing_percentage > threshold]\n",
    "\n",
    "print(f\"Number of rows to remove: {len(rows_to_remove)}\")\n",
    "print(\"Columns to remove:\", list(columns_to_remove.index))\n",
    "\n",
    "#If no row in the dataset has more than 30% missing values, the output will be 0\n",
    "#If all columns have less than 30% missing values, the output will be: []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing rows with excessive missing values ensures that observations (data points) with incomplete information don’t bias or degrade analysis.\n",
    "Removing columns with excessive missing values ensures that features (variables) with insufficient data don’t negatively impact model training or insights.\n",
    "\n",
    "be stricter or more lenient, adjust the threshold:\n",
    "Higher Threshold: Fewer rows/columns removed, tolerating more missing data.\n",
    "Lower Threshold: More rows/columns removed, demanding more complete data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable distribution:\n",
      "categorical_var\n",
      "High          90.321285\n",
      "Low            9.668675\n",
      "-999999999     0.010040\n",
      "Name: proportion, dtype: float64\n",
      "The target variable is imbalanced. Classes with low representation:\n",
      "categorical_var\n",
      "Low           9.668675\n",
      "-999999999    0.010040\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Is the target variable well-balanced?\n",
    "#Analyze the distribution of the target variable\n",
    "#target variable is in the last column of the dataset.\n",
    "#Calculates the proportion of each class in the target variable as a percentage.\n",
    "#normalize=True ensures the values are fractions\n",
    "\n",
    "\n",
    "target_variable = data.columns[-1]\n",
    "target_distribution = data[target_variable].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Target variable distribution:\")\n",
    "print(target_distribution)\n",
    "# Sets the threshold for imbalance (e.g., 10%).\n",
    "#If any class has less than 10% representation, it's flagged as imbalanced.\n",
    "# Determine if it's balanced (e.g., no class <10% or >90%)\n",
    "balance_threshold = 10  # Example: Consider imbalance if a class is <10%\n",
    "imbalanced = target_distribution[target_distribution < balance_threshold]\n",
    "# If no class meets the imbalance condition, the target variable is considered well-balanced.\n",
    "if imbalanced.empty:\n",
    "    print(\"The target variable is well-balanced.\")\n",
    "else:\n",
    "    print(f\"The target variable is imbalanced. Classes with low representation:\\n{imbalanced}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold for Balance:\n",
    "\n",
    "The choice of balance_threshold depends on your specific use case:\n",
    "Strict: 10% is common. Classes with less than 10% representation are flagged as imbalanced.\n",
    "Lenient: Thresholds like 5% or 20% can be used for stricter or looser definitions of balance.\n",
    "\n",
    "Well-Balanced Target Variable:\n",
    "\n",
    "All classes are adequately represented.\n",
    "The model learns from all classes and avoids bias toward the majority class.\n",
    "Example: Binary classification with a 50/50 split between classes.\n",
    "Imbalanced Target Variable:\n",
    "\n",
    "If one or more classes have very low representation, the model might:\n",
    "Struggle to learn from underrepresented classes.\n",
    "Predict the majority class most of the time (e.g., always predicting \"High\" in a 90% \"High\" dataset).\n",
    "\n",
    "The target variable is imbalanced. Classes with low representation:\n",
    "Low           9.67\n",
    "-999999999    0.01\n",
    "This indicates the target variable is dominated by \"High\" and lacks sufficient representation for \"Low\" and -999999999.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Results:\n",
    "Missing Data\n",
    "\n",
    "Percentage of missing values: Each column has a small amount of missing data (around 0.39%-0.40%). This is negligible in a dataset of 10,000 rows, so dropping rows or columns for this level of missingness isn't necessary.\n",
    "No rows or columns flagged for removal: This aligns with the minimal missingness.\n",
    "Outliers and Invalid Values\n",
    "\n",
    "The data contains extreme and likely invalid values:\n",
    "numerical_var_1: A value of -1.000000e+09 (likely an invalid placeholder or error).\n",
    "numerical_var_2: Similarly contains -1.000000e+09.\n",
    "categorical_var: Includes -999999999, which seems invalid.\n",
    "These invalid data points need to be addressed by replacing or removing them.\n",
    "\n",
    "Target Variable\n",
    "\n",
    "The target variable is heavily imbalanced:\n",
    "Class \"High\": 90.32%.\n",
    "Class \"Low\": 9.67%.\n",
    "Class \"-999999999\": 0.01% (invalid and needs to be removed).\n",
    "This imbalance suggests that resampling (e.g., oversampling \"Low\" or undersampling \"High\") may be needed for balanced training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after dropping missing values: 9801\n"
     ]
    }
   ],
   "source": [
    "#Clean the dataset by handling missing values and removing any invalid data points.\n",
    "# .dropna() is used to remove rows that have any missing values\n",
    "\n",
    "cleaned_data = data.dropna()\n",
    "print(f\"Rows after dropping missing values: {len(cleaned_data)}\")\n",
    "# The percentage of missing values is low (about 0.4%), so removing a small fraction of rows won't significantly impact the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning invalid values: 9800\n"
     ]
    }
   ],
   "source": [
    "# Replace invalid numeric values with NaN, then drop these rows\n",
    "# Define invalid values for numeric and categorical variables\n",
    "invalid_numeric = -1.0e+09\n",
    "invalid_categorical = \"-999999999\"\n",
    "\n",
    "# Replace invalid numeric values with NaN and drop these rows\n",
    "cleaned_data = cleaned_data.replace(invalid_numeric, pd.NA)\n",
    "cleaned_data = cleaned_data.dropna()\n",
    "\n",
    "# Remove rows with invalid categorical values\n",
    "cleaned_data = cleaned_data[cleaned_data['categorical_var'] != invalid_categorical]\n",
    "\n",
    "# Remove duplicate rows (if any)\n",
    "cleaned_data = cleaned_data.drop_duplicates()\n",
    "\n",
    "print(f\"Rows after cleaning invalid values: {len(cleaned_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.1\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\marya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\marya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\marya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\marya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\marya\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Class distribution after balancing:\n",
      "categorical_var\n",
      "High    8848\n",
      "Low     8848\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# resample function is used to create upsampled or downsampled datasets.\n",
    "!pip install scikit-learn\n",
    "\n",
    "from sklearn.utils import resample\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "# Separate classes dynamically\n",
    "class_counts = cleaned_data['categorical_var'].value_counts()#Computes the count of each unique value in the categorical_var column.\n",
    "majority_class = class_counts.idxmax()  # Identifies the category with the maximum count (i.e., the majority class).\n",
    "#Filters the dataset to include only rows where categorical_var equals the majority class.\n",
    "majority = cleaned_data[cleaned_data['categorical_var'] == majority_class]\n",
    "\n",
    "# Balance all other classes against the majority class\n",
    "balanced_data = majority.copy()#Makes a copy of the majority class dataset to initialize balanced_data\n",
    "\n",
    "for category in class_counts.index: #Iterates over each category (e.g., \"High\", \"Low\", \"Medium\") in the categorical_var column.\n",
    "    if category != majority_class:#Ensures that the loop skips the majority class, as it's already included in balanced_data.\n",
    "        category_data = cleaned_data[cleaned_data['categorical_var'] == category]#Filters the dataset to include only rows belonging to the current category (e.g., \"Low\"\n",
    "        #Upsamples the category_data to match the size of the majority class (len(majority)):\n",
    "        upsampled = resample(\n",
    "            category_data,\n",
    "            replace=True,#Allows sampling with replacement, enabling duplicates in the upsampled dataset.\n",
    "            n_samples=len(majority),  # Sets the size of the upsampled dataset to equal the majority class size.\n",
    "            random_state=42\n",
    "        )\n",
    "        #Appends the upsampled category data to the balanced_data dataframe.\n",
    "        balanced_data = pd.concat([balanced_data, upsampled])\n",
    "\n",
    "# sampling 100% of the rows in a randomized order.\n",
    "# Shuffle the dataset after balancing\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Displays the count of each category in the balanced dataset to confirm all classes have the same size.\n",
    "# Check class distribution\n",
    "print(\"Class distribution after balancing:\")\n",
    "print(balanced_data['categorical_var'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test subsets saved as independent files.\n"
     ]
    }
   ],
   "source": [
    "#imports the train_test_split function for splitting the dataset.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X: Contains all columns except categorical_var, which represents the features.\n",
    "# Contains only the categorical_var column, which is the target.\n",
    "# Define features (X) and target (y)\n",
    "X = balanced_data.drop(columns='categorical_var')\n",
    "y = balanced_data['categorical_var']\n",
    "\n",
    "# test_size=0.2: Allocates 20% of the dataset for testing.\n",
    "# \n",
    "# Split into training and test sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# X_train and X_test are saved as X_train.csv and X_test.csv.\n",
    "# y_train and y_test are saved as y_train.csv and y_test.csv.\n",
    "# Save subsets as CSV files\n",
    "X_train.to_csv(\"X_train.csv\", index=False)\n",
    "X_test.to_csv(\"X_test.csv\", index=False)\n",
    "y_train.to_csv(\"y_train.csv\", index=False)\n",
    "y_test.to_csv(\"y_test.csv\", index=False)\n",
    "\n",
    "print(\"Training and test subsets saved as independent files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
