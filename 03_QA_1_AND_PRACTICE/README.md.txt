# Machine Learning Projects ‚Äì Model Selection, Regularization, and Custom Loss Functions

This collection of exercises was completed as part of my **AI and Data Science teaching and research activities** at **EDHEC Business School (Nice, France)**.  
Each notebook explores a different aspect of **model selection, regression, and optimization**, emphasizing reproducible experimentation and analytical reasoning.

---

## üìò Overview

These projects cover a progressive pipeline of **supervised learning** concepts ‚Äî from basic model fitting to advanced regularization and custom loss optimization.

| File | Focus | Description |
|------|-------|-------------|
| **1_Model_Selection_small.pdf** | Linear Model Selection (Small Dataset) | Building and evaluating linear regression models using MSE, AIC, BIC, and R¬≤. |
| **2_Model_Selection_big.pdf** | Model Selection with Quadratic Terms | Exploring polynomial and interaction effects to capture non-linear relationships. |
| **3_Non_Linear.pdf** | Polynomial Regression | Selecting optimal polynomial degree based on validation MSE, balancing bias and variance. |
| **4_Regularization.pdf** | Ridge, Lasso & Elastic Net | Comparing regularization techniques for feature selection, interpretability, and performance. |
| **5_custom_loss.pdf** | Custom Loss Optimization | Implementing gradient descent to minimize an asymmetric (weighted) loss function. |

---

## üß† Learning Objectives

- Understand **model selection** and **evaluation metrics** (AIC, BIC, MSE, R¬≤).  
- Identify and mitigate **overfitting** using regularization and cross-validation.  
- Apply **polynomial feature expansion** for non-linear data.  
- Implement and compare **Lasso**, **Ridge**, and **Elastic Net** regression.  
- Design and minimize a **custom loss function** via **gradient descent**.  
- Develop structured, explainable, and reproducible ML pipelines.

---

## üß∞ Tools and Libraries

- **Python 3**
- **pandas**, **NumPy**
- **scikit-learn**, **statsmodels**
- **matplotlib**, **seaborn**
- **Jupyter Notebook**

---

## üî¨ Methodological Highlights

- **Exploratory Data Analysis (EDA)** ‚Äî correlation matrices, summary statistics, outlier detection.  
- **Train/Validation/Test Splits** ‚Äî robust evaluation with optional cross-validation.  
- **Model Comparison** ‚Äî analytical and statistical criteria for model choice.  
- **Regularization Path Analysis** ‚Äî monitoring feature sparsity vs. accuracy trade-off.  
- **Custom Loss Implementation** ‚Äî asymmetry handling in regression under uncertainty.  

---

## üí° Key Insights

- Effective model selection depends on balancing **accuracy**, **complexity**, and **interpretability**.  
- Regularization techniques improve **generalization** and **robustness**.  
- Custom loss functions provide flexibility for **domain-specific optimization**, such as risk or cost-sensitive prediction.  

---

## üìä Relevance

These exercises reinforce technical skills in **applied machine learning** and **model optimization**‚Äîdirectly applicable to domains like:  
- **Cyber-risk analytics**  
- **Financial forecasting**  
- **Operational AI systems**  

The combination of explainable models and rigorous validation aligns with the goals of organizations such as **Reciproc-IT**, focusing on **data-driven risk management** and **trustworthy AI**.

---

## üë©‚Äçüíª Author

**Maryam Madani**  
MSc Digital Security ‚Äì EURECOM (Sophia Antipolis)  
Teaching Assistant ‚Äì AI & Data Science, EDHEC Business School  
[GitHub Profile](https://github.com/madani2015)
